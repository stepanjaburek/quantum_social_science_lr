{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stepanjaburek/quantum_social_science_lr/blob/main/permutated_tests_colab_HCPC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantum Social Science: PCA and Cluster Analysis of the Literature"
      ],
      "metadata": {
        "id": "5lj-bQwjntRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "ugbp61azb7Il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn-extra\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from IPython.display import display"
      ],
      "metadata": {
        "id": "_BTdRu2DyipG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # this asks for your google account\n",
        "# load data from our private google folder shared among us\n",
        "data = pd.read_excel(\"/content/drive/MyDrive/QSS_Colab/keyword_data.xlsx\")\n",
        "metadata = pd.read_excel(\"/content/drive/MyDrive/QSS_Colab/metadata.xlsx\")\n",
        "data1 = pd.read_excel(\"/content/drive/MyDrive/QSS_Colab/1_dataset_with_all_21_variables.xlsx\")\n",
        "data2 = pd.read_excel(\"/content/drive/MyDrive/QSS_Colab/2_dataset_with_just_6_variables.xlsx\")\n",
        "data3 = pd.read_excel(\"/content/drive/MyDrive/QSS_Colab/3_permutation_in_columns.xlsx\")\n",
        "data4 = pd.read_excel(\"/content/drive/MyDrive/QSS_Colab/4_permutation_in_columns_6_stays.xlsx\")\n",
        "data5 = pd.read_excel(\"/content/drive/MyDrive/QSS_Colab/5_permutation_in_columns_15_stays.xlsx\")\n",
        "data6 = pd.read_excel(\"/content/drive/MyDrive/QSS_Colab/6_permutation_in_rows.xlsx\")"
      ],
      "metadata": {
        "id": "nh0WYoeYqLEZ",
        "outputId": "d4239a8a-2381-4fa1-b464-b627c4a63d8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data cleaning"
      ],
      "metadata": {
        "id": "0u69g6d6n1mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename \"Broekaert_2018\" in metadata\n",
        "metadata.loc[252, 'file_name'] = \"Broekaert_2018_The Tacit 'Quantum' of Meeting the Aesthetic Sign; Contextualize, Entangle,2.pdf\"\n",
        "data1.loc[252, 'filename'] = \"Broekaert_2018_The Tacit 'Quantum' of Meeting the Aesthetic Sign; Contextualize, Entangle,2.pdf\"\n",
        "# Delete duplicit paper \"Yukalov et al. - 2018 - Information processing by networks of quantum deci.pdf\"\n",
        "data1 = data1.drop(1170)\n",
        "# Delete paper \"Yilmaz - 2017 - Quantum cognition models of ethical decision-makin.pdf\" that is only present in data\n",
        "data1 = data1.drop(1166)\n",
        "# Delete paper \"Park_2016_Decision-making &amp quantum mechanical models of cognitive processing.pdf\" that is only present in metadata\n",
        "metadata = metadata.drop(892)"
      ],
      "metadata": {
        "id": "LjkkYUF4i9DN"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2.loc[252, 'filename'] = \"Broekaert_2018_The Tacit 'Quantum' of Meeting the Aesthetic Sign; Contextualize, Entangle,2.pdf\"\n",
        "# Delete duplicit paper \"Yukalov et al. - 2018 - Information processing by networks of quantum deci.pdf\"\n",
        "data2 = data2.drop(1170)\n",
        "# Delete paper \"Yilmaz - 2017 - Quantum cognition models of ethical decision-makin.pdf\" that is only present in data\n",
        "data2 = data2.drop(1166)"
      ],
      "metadata": {
        "id": "I5bl_WF7swj7"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data3.loc[252, 'filename'] = \"Broekaert_2018_The Tacit 'Quantum' of Meeting the Aesthetic Sign; Contextualize, Entangle,2.pdf\"\n",
        "# Delete duplicit paper \"Yukalov et al. - 2018 - Information processing by networks of quantum deci.pdf\"\n",
        "data3 = data3.drop(1170)\n",
        "# Delete paper \"Yilmaz - 2017 - Quantum cognition models of ethical decision-makin.pdf\" that is only present in data\n",
        "data3 = data3.drop(1166)"
      ],
      "metadata": {
        "id": "9tvy2xrgt8Vi"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data4.loc[252, 'filename'] = \"Broekaert_2018_The Tacit 'Quantum' of Meeting the Aesthetic Sign; Contextualize, Entangle,2.pdf\"\n",
        "# Delete duplicit paper \"Yukalov et al. - 2018 - Information processing by networks of quantum deci.pdf\"\n",
        "data4 = data4.drop(1170)\n",
        "# Delete paper \"Yilmaz - 2017 - Quantum cognition models of ethical decision-makin.pdf\" that is only present in data\n",
        "data4 = data4.drop(1166)"
      ],
      "metadata": {
        "id": "IgYlPRsztbr5"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data5.loc[252, 'filename'] = \"Broekaert_2018_The Tacit 'Quantum' of Meeting the Aesthetic Sign; Contextualize, Entangle,2.pdf\"\n",
        "# Delete duplicit paper \"Yukalov et al. - 2018 - Information processing by networks of quantum deci.pdf\"\n",
        "data5 = data5.drop(1170)\n",
        "# Delete paper \"Yilmaz - 2017 - Quantum cognition models of ethical decision-makin.pdf\" that is only present in data\n",
        "data5 = data5.drop(1166)"
      ],
      "metadata": {
        "id": "MHT6Lmc6u0dU"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data6.loc[252, 'filename'] = \"Broekaert_2018_The Tacit 'Quantum' of Meeting the Aesthetic Sign; Contextualize, Entangle,2.pdf\"\n",
        "# Delete duplicit paper \"Yukalov et al. - 2018 - Information processing by networks of quantum deci.pdf\"\n",
        "data6 = data6.drop(1170)\n",
        "# Delete paper \"Yilmaz - 2017 - Quantum cognition models of ethical decision-makin.pdf\" that is only present in data\n",
        "data6 = data6.drop(1166)"
      ],
      "metadata": {
        "id": "Opvm92sHvdyq"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=data1"
      ],
      "metadata": {
        "id": "RAr6B5AnfBfL"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=data1\n",
        "metadata.insert(0, 'id', range(1, len(metadata) + 1))\n",
        "df.insert(0, 'id', range(1, len(df) + 1))"
      ],
      "metadata": {
        "id": "SxjMaroHopfY"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "ow98Iijhoq-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keywords"
      ],
      "metadata": {
        "id": "2l_j_UngjBe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df.iloc[:, 2:23].sum(axis=1) > 0]\n",
        "df = df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "DIBTi93nmysg"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[(i, list(df.keys())[i]) for i in range(len(df.keys()))]"
      ],
      "metadata": {
        "id": "IjE0C6BJ-1ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scaling in the rows"
      ],
      "metadata": {
        "id": "0C0uUN2ocb5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create features for the pca\n",
        "features = df.drop(['filename', 'id'], axis=1)\n",
        "feature_names = features.columns\n",
        "\n",
        "features = features.div(features.sum(axis=1), axis=0) # row normalization as Michael suggested, dividing the values by the sum of their row (paper)\n",
        "#features = StandardScaler().fit_transform(features.T).T # Z-score standardization on transposed data to work in rows\n",
        "\n",
        "features = pd.DataFrame(features, columns=feature_names)\n",
        "features"
      ],
      "metadata": {
        "id": "0WWcK4vtTneT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data analysis"
      ],
      "metadata": {
        "id": "b8EjIT_YcyL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PCA"
      ],
      "metadata": {
        "id": "L9w7jKKnoH6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA()\n",
        "pca_result = pca.fit_transform(features)\n",
        "print(\"Explained variance ratio:\", pca.explained_variance_ratio_[:10])\n",
        "\n",
        "# Scree plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n",
        "         pca.explained_variance_ratio_, 'bo-')\n",
        "plt.title('Scree Plot of Principal Components')\n",
        "plt.xlabel('Principal Components')\n",
        "plt.ylabel('Percentage of explained variances')\n",
        "plt.show()\n",
        "\n",
        "# Feature importance in components\n",
        "loadings = pd.DataFrame(\n",
        "    pca.components_.T,\n",
        "    columns=[f'PC{i+1}' for i in range(len(pca.components_))],\n",
        "    index=features.columns\n",
        ")\n",
        "\n",
        "# Top 5 features per component\n",
        "top_loadings = pd.DataFrame()\n",
        "for pc in loadings.columns:\n",
        "    top_5 = pd.DataFrame({\n",
        "        'feature': loadings.index,\n",
        "        'PC': pc,\n",
        "        'loading': loadings[pc]\n",
        "    })\n",
        "\n",
        "    top_5 = top_5.reindex(top_5['loading'].abs().sort_values(ascending=False).index)\n",
        "    top_5 = top_5.head(5)\n",
        "    top_loadings = pd.concat([top_loadings, top_5])\n",
        "\n",
        "print(top_loadings)\n",
        "\n",
        "# PC1 and 2\n",
        "pc12_loadings = top_loadings[top_loadings['PC'].isin(['PC1', 'PC2', 'PC3', 'PC4'])]\n",
        "pc12_loadings = pc12_loadings.sort_values(['PC', 'loading'],\n",
        "                                         ascending=[True, False])\n",
        "print(pc12_loadings)\n",
        "\n",
        "# PCA biplot\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.5)\n",
        "\n",
        "for i, feature in enumerate(features.columns):\n",
        "    plt.arrow(0, 0,\n",
        "              pca.components_[0, i]*max(abs(pca_result[:, 0])),\n",
        "              pca.components_[1, i]*max(abs(pca_result[:, 1])),\n",
        "              color='r', alpha=0.5)\n",
        "    plt.text(pca.components_[0, i]*max(abs(pca_result[:, 0]))*1.15,\n",
        "             pca.components_[1, i]*max(abs(pca_result[:, 1]))*1.15,\n",
        "             feature)\n",
        "\n",
        "plt.xlabel('First Principal Component')\n",
        "plt.ylabel('Second Principal Component')\n",
        "plt.title('PCA of Paper Concepts')\n",
        "plt.show()\n",
        "display(pc12_loadings)"
      ],
      "metadata": {
        "id": "PF34txR9AW2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many PCs do we want? Literature says probably a sum between 80-90% of explained variance"
      ],
      "metadata": {
        "id": "zOzKjNdhMRSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
        "# Find number of components needed for 80% variance (we can change this)\n",
        "n_components_80 = np.argmax(cumulative_variance_ratio >= 0.8) + 1\n",
        "\n",
        "print(f\"Number of components needed to explain 80% of variance: {n_components_80}\")\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(cumulative_variance_ratio) + 1),\n",
        "         cumulative_variance_ratio,\n",
        "         'bo-')\n",
        "plt.axhline(y=0.8, color='r', linestyle='--', label='80% Threshold')\n",
        "plt.axvline(x=n_components_80, color='g', linestyle='--',\n",
        "            label=f'Components needed: {n_components_80}')\n",
        "plt.title('Cumulative Explained Variance Ratio')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nCumulative explained variance for first 10 components:\")\n",
        "for i in range(10):\n",
        "    print(f\"Components 1-{i+1}: {cumulative_variance_ratio[i]:.3f}\")"
      ],
      "metadata": {
        "id": "7dFNNsTQFyRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hierarchical Clustering on Principal Components (HCPC)"
      ],
      "metadata": {
        "id": "aYBuz43aIZQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
        "\n",
        "# 1. Define how many PCs to use\n",
        "n_components = 7 # I think we should use between 7 and 9 based on the PCA\n",
        "selected_components = pca_result[:, :n_components]\n",
        "\n",
        "# 2. Hierarchical clustering uisng Ward\n",
        "linked = linkage(selected_components, method='ward') # the methods can be different. Ward method is the one we use in Ipsos and seemingly the typical choice in the literature\n",
        "\n",
        "# Plot dendrogram\n",
        "plt.figure(figsize=(12, 8))\n",
        "dendrogram(linked, truncate_mode='lastp', p=12)\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Distance')\n",
        "plt.show()\n",
        "\n",
        "# 3. Get initial clusters from hierarchical clustering # optional step though, we dont really need them\n",
        "n_clusters = 4\n",
        "hc_labels = fcluster(linked, n_clusters, criterion='maxclust')\n",
        "\n",
        "# 4. K-means to finalize things\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "kmeans.fit(selected_components)\n",
        "final_labels = kmeans.labels_\n",
        "\n",
        "\n",
        "# Visualize final clusters\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1],\n",
        "                     c=final_labels, cmap='viridis')\n",
        "centroids = kmeans.cluster_centers_\n",
        "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
        "           c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
        "plt.colorbar(scatter)\n",
        "plt.title('HCPC Final Clusters')\n",
        "plt.xlabel('First Principal Component')\n",
        "plt.ylabel('Second Principal Component')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EMLnGW5fIvE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Medoids as an alternative ending"
      ],
      "metadata": {
        "id": "zQfUjvfQbmn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmedoids = KMedoids(n_clusters=n_clusters,\n",
        "                    random_state=42,\n",
        "                    metric='euclidean')\n",
        "kmedoids.fit(selected_components)\n",
        "final_labels = kmedoids.labels_\n",
        "\n",
        "# Visualize final clusters\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1],\n",
        "                     c=final_labels, cmap='viridis')\n",
        "\n",
        "# Get medoid locations in PCA space\n",
        "medoid_indices = kmedoids.medoid_indices_\n",
        "medoid_locations = pca_result[medoid_indices]\n",
        "\n",
        "# Plot medoids\n",
        "plt.scatter(medoid_locations[:, 0], medoid_locations[:, 1],\n",
        "           c='red', marker='x', s=200, linewidths=3, label='Medoids')\n",
        "\n",
        "plt.colorbar(scatter)\n",
        "plt.title('HCPC Final Clusters (K-medoids)')\n",
        "plt.xlabel('First Principal Component')\n",
        "plt.ylabel('Second Principal Component')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ybGoRg4lbAhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get central papers from the HCPC"
      ],
      "metadata": {
        "id": "BRzBioJdaGj1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kmeans"
      ],
      "metadata": {
        "id": "0nJZcGtI7_-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "centroids_list = []\n",
        "for i in range(n_clusters):\n",
        "    distances = np.sum((selected_components - kmeans.cluster_centers_[i])**2, axis=1)\n",
        "    top_indices = np.argsort(distances)[:10] # N of central papers\n",
        "    pca_coords = pca_result[top_indices]\n",
        "\n",
        "    feature_importance = np.dot(pca_coords[:, :7], pca.components_[:7, :])\n",
        "    feature_importance_df = pd.DataFrame(\n",
        "        feature_importance,\n",
        "        columns=features.columns\n",
        "    )\n",
        "\n",
        "    # Get top 5 most important features for each paper\n",
        "    top_features = pd.DataFrame({\n",
        "        'paper_id': df['id'].iloc[top_indices].values,\n",
        "        'top_features': [\n",
        "            ', '.join(feature_importance_df.iloc[j].nlargest(5).index.tolist())\n",
        "            for j in range(len(feature_importance_df))\n",
        "        ]\n",
        "    })\n",
        "        # Get PCA coordinates for the top papers, here we work with the 7 PCs, we can change it\n",
        "    cluster_papers = pd.DataFrame({\n",
        "        'cluster': [i + 1] * 10,\n",
        "        'rank': range(1, 11),\n",
        "        'centroid_id': df['id'].iloc[top_indices].values,\n",
        "        'distance': distances[top_indices],\n",
        "        'PC1': pca_coords[:, 0],\n",
        "        'PC2': pca_coords[:, 1],\n",
        "        'PC3': pca_coords[:, 2],\n",
        "        'PC4': pca_coords[:, 3],\n",
        "        'PC5': pca_coords[:, 4],\n",
        "        'PC6': pca_coords[:, 5],\n",
        "        'PC7': pca_coords[:, 6],\n",
        "        'key_features': top_features['top_features']\n",
        "    })\n",
        "    centroids_list.append(cluster_papers)\n",
        "\n",
        "centroids = pd.concat(centroids_list, ignore_index=True)\n",
        "centerpapers = pd.merge(centroids, metadata, left_on='centroid_id', right_on='id', how='left')\n",
        "centerpapers.to_csv('means_centroids_no_operator.csv', index=False)"
      ],
      "metadata": {
        "id": "kNrUtow2KsGL"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "centerpapers"
      ],
      "metadata": {
        "id": "sKBPRhLVxGwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KMedoids"
      ],
      "metadata": {
        "id": "GhftAeur8CKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "centroids_list = []\n",
        "for i in range(n_clusters):\n",
        "    medoid_point = selected_components[kmedoids.medoid_indices_[i]]\n",
        "    distances = np.sum((selected_components - medoid_point)**2, axis=1)\n",
        "    top_indices = np.argsort(distances)[:10]  # N of central papers\n",
        "    pca_coords = pca_result[top_indices]\n",
        "\n",
        "    feature_importance = np.dot(pca_coords[:, :7], pca.components_[:7, :])\n",
        "    feature_importance_df = pd.DataFrame(\n",
        "        feature_importance,\n",
        "        columns=features.columns\n",
        "    )\n",
        "\n",
        "    # Get top 5 most important features for each paper\n",
        "    top_features = pd.DataFrame({\n",
        "        'paper_id': df['id'].iloc[top_indices].values,\n",
        "        'top_features': [\n",
        "            ', '.join(feature_importance_df.iloc[j].nlargest(5).index.tolist())\n",
        "            for j in range(len(feature_importance_df))\n",
        "        ]\n",
        "    })\n",
        "     # Get PCA coordinates for the top papers, here we work with the 8 PCs, we can change it\n",
        "    cluster_papers = pd.DataFrame({\n",
        "        'cluster': [i + 1] * 10,\n",
        "        'rank': range(1, 11),\n",
        "        'centroid_id': df['id'].iloc[top_indices].values,\n",
        "        'distance': distances[top_indices],\n",
        "        'PC1': pca_coords[:, 0],\n",
        "        'PC2': pca_coords[:, 1],\n",
        "        'PC3': pca_coords[:, 2],\n",
        "        'PC4': pca_coords[:, 3],\n",
        "        'PC5': pca_coords[:, 4],\n",
        "        'PC6': pca_coords[:, 5],\n",
        "        'PC7': pca_coords[:, 6],\n",
        "        'key_features': top_features['top_features']\n",
        "    })\n",
        "    centroids_list.append(cluster_papers)\n",
        "\n",
        "centroids = pd.concat(centroids_list, ignore_index=True)\n",
        "centerpapers = pd.merge(centroids, metadata, left_on='centroid_id', right_on='id', how='left')\n",
        "centerpapers.to_csv('medoids_centroids_no_operator.csv', index=False)"
      ],
      "metadata": {
        "id": "H2aLajvJ6K0P"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clustered_df = df.copy()\n",
        "clustered_df['cluster'] = final_labels + 1\n",
        "\n",
        "clustered_df.to_csv('papers_with_clusters.csv', index=False)"
      ],
      "metadata": {
        "id": "YEBHJxiy-qMp"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Means vs K-Medoids"
      ],
      "metadata": {
        "id": "fT0r1UMngIaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score, normalized_mutual_info_score\n",
        "kmeans_labels = kmeans.labels_\n",
        "kmedoids_labels = kmedoids.labels_\n",
        "\n",
        "# similarity metrics\n",
        "ari = adjusted_rand_score(kmeans_labels, kmedoids_labels)\n",
        "ami = adjusted_mutual_info_score(kmeans_labels, kmedoids_labels)\n",
        "nmi = normalized_mutual_info_score(kmeans_labels, kmedoids_labels)\n",
        "\n",
        "print(f\"Adjusted Rand Index: {ari:.3f}\")\n",
        "print(f\"Adjusted Mutual Information: {ami:.3f}\")\n",
        "print(f\"Normalized Mutual Information: {nmi:.3f}\")\n",
        "\n",
        "confusion_matrix = pd.crosstab(\n",
        "    kmeans_labels,\n",
        "    kmedoids_labels,\n",
        "    margins=True,\n",
        "    normalize='index'\n",
        ")\n",
        "print(\"\\nConfusion Matrix (normalized by row):\")\n",
        "print(confusion_matrix)\n",
        "\n",
        "# Visual comparison\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# K-means plot\n",
        "scatter1 = ax1.scatter(pca_result[:, 0], pca_result[:, 1], c=kmeans_labels, cmap='viridis')\n",
        "ax1.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "           c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
        "ax1.set_title('K-means Clusters')\n",
        "ax1.set_xlabel('First Principal Component')\n",
        "ax1.set_ylabel('Second Principal Component')\n",
        "ax1.legend()\n",
        "\n",
        "# K-medoids plot\n",
        "scatter2 = ax2.scatter(pca_result[:, 0], pca_result[:, 1], c=kmedoids_labels, cmap='viridis')\n",
        "ax2.scatter(medoid_locations[:, 0], medoid_locations[:, 1],\n",
        "           c='red', marker='x', s=200, linewidths=3, label='Medoids')\n",
        "ax2.set_title('K-medoids Clusters')\n",
        "ax2.set_xlabel('First Principal Component')\n",
        "ax2.set_ylabel('Second Principal Component')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1PHb9asjgGxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Somewhat good overlap of K-Means and K-Medoids with 7 PCs and 4 clusters. ARI=0.577; C1 (95.2%), C2 (94.4%), C3 (98.8%), C4 (38%) and (61.4%)"
      ],
      "metadata": {
        "id": "oU2Zfl1Gis_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Very good overlap of K-Means and K-Medoids with 9 PCs and 4 clusters. ARI=0.77; C1 (80.4% overlap), C2(88.5% overlap), C3 (95.6% overlap), C4 (100% overlap)"
      ],
      "metadata": {
        "id": "lAoS2ncAh-TX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8DW2MzIcD09k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest for a ML approach to \"predicting\" the clusters we got from the PCA a Cluster Analysis with the just the original (normalized) keywords.\n",
        "\n"
      ],
      "metadata": {
        "id": "Lxp6nedjDDvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gives us the same picture as the PCA = Operator, Uncertainty, and Entanglement predict one cluster each, the fourth is predicted by a mix of words. Not sure we get any useful info here. **But the model fits extremely well all clusters from the original data, this is good robustness for the pca and clsuter approach we have.**"
      ],
      "metadata": {
        "id": "A3kj_Ww1D1rp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X = features\n",
        "y = final_labels + 1\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': features.columns,\n",
        "    'importance': rf.feature_importances_\n",
        "})\n",
        "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
        "\n",
        "# 20 most important features\n",
        "print(\"Top 20 most important features:\")\n",
        "print(feature_importance.head(20))\n",
        "\n",
        "# Model performance\n",
        "y_pred = rf.predict(X_test)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# viz\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='importance', y='feature', data=feature_importance.head(20))\n",
        "plt.title('Top 20 Most Important Features for Cluster Prediction')\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZL-QbNsBAraw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for cluster_num in range(1, 5):\n",
        "    print(f\"\\n=== ANALYSIS FOR CLUSTER {cluster_num} ===\")\n",
        "\n",
        "    binary_labels = (final_labels + 1 == cluster_num).astype(int)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        features, binary_labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "    rf = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': features.columns,\n",
        "        'importance': rf.feature_importances_\n",
        "    })\n",
        "    feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
        "\n",
        "    ################################################\n",
        "\n",
        "    # top 10 features for this cluster\n",
        "    print(f\"\\nTop 10 most important features for Cluster {cluster_num}:\")\n",
        "    print(feature_importance.head(10))\n",
        "\n",
        "    # Model performance\n",
        "    y_pred = rf.predict(X_test)\n",
        "    print(f\"\\nClassification Report for Cluster {cluster_num}:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # viz\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.barplot(x='importance', y='feature', data=feature_importance.head(20))\n",
        "    plt.title(f'Top 20 Most Important Features for Cluster {cluster_num}')\n",
        "    plt.xlabel('Feature Importance')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "vdxuWttnA_x1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}