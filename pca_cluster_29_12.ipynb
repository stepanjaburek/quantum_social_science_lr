{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMx+8+TAq8UYFEDy4jrTh2P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stepanjaburek/quantum_social_science_lr/blob/main/pca_cluster_29_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup and data wrangling"
      ],
      "metadata": {
        "id": "ugbp61azb7Il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################################\n",
        "# PCA and cluster analysis\n",
        "###############################\n",
        "# Setup\n",
        "########\n",
        "!pip install scikit-learn-extra\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "\n",
        "data = pd.read_csv(\"/content/0_kw_analysis.csv\")\n",
        "metadata = pd.read_excel(\"/content/pdf_list(1).xlsx\")"
      ],
      "metadata": {
        "id": "_BTdRu2DyipG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############################\n",
        "# Data cleaning\n",
        "###########################\n",
        "# Rename \"Broekaert_2018\" in metadata\n",
        "metadata.loc[252, 'file_name'] = \"Broekaert_2018_The Tacit 'Quantum' of Meeting the Aesthetic Sign; Contextualize, Entangle,2.pdf\"\n",
        "data.loc[252, 'filename'] = \"Broekaert_2018_The Tacit 'Quantum' of Meeting the Aesthetic Sign; Contextualize, Entangle,2.pdf\"\n",
        "# Delete duplicit paper \"Yukalov et al. - 2018 - Information processing by networks of quantum deci.pdf\"\n",
        "data = data.drop(1170)\n",
        "# Delete paper \"Yilmaz - 2017 - Quantum cognition models of ethical decision-makin.pdf\" that is only present in data\n",
        "data = data.drop(1166)\n",
        "# Delete paper \"Park_2016_Decision-making &amp quantum mechanical models of cognitive processing.pdf\" that is only present in metadata\n",
        "metadata = metadata.drop(892)\n",
        "\n",
        "# Exclude authors and create df\n",
        "df = df.drop(columns=[df.columns[i] for i in [1,2,4,16,30]])\n",
        "# Exclude social science fields\n",
        "df = df.drop(columns=df.columns[26:38])\n",
        "# Possibly also Exclude \"quantum\" and \"quantization\"\n",
        "df = df.drop(columns=df.columns[17:19])\n",
        "\n",
        "# keep only rows with some non-zero values\n",
        "df = df[df.iloc[:, 2:24].sum(axis=1) > 0]\n",
        "df = df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "jKsA4rIryes4"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata = metadata.reset_index()\n",
        "metadata = metadata.rename(columns={'index': 'id'})\n",
        "metadata['id'] = metadata['id'] + 1\n",
        "\n",
        "df = df.reset_index()\n",
        "df = df.rename(columns={'index': 'id'})\n",
        "df['id'] = df['id'] + 1"
      ],
      "metadata": {
        "id": "X5h0PIS6iknZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Standardization"
      ],
      "metadata": {
        "id": "0C0uUN2ocb5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create features for the pca\n",
        "features = df.drop(['filename', 'id'], axis=1)\n",
        "feature_names = features.columns\n",
        "\n",
        "# scale with either z-score standardization or MinMax\n",
        "features = StandardScaler().fit_transform(features) # z score\n",
        "#features = MinMaxScaler().fit_transform(features) # minmax\n",
        "\n",
        "features = pd.DataFrame(features, columns=feature_names)\n",
        "features"
      ],
      "metadata": {
        "id": "2e2XrUxg5GV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis"
      ],
      "metadata": {
        "id": "b8EjIT_YcyL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################\n",
        "# Data analysis\n",
        "#####################\n",
        "# PCA\n",
        "pca = PCA()\n",
        "pca_result = pca.fit_transform(features)\n",
        "print(\"Explained variance ratio:\", pca.explained_variance_ratio_[:5])\n",
        "\n",
        "# Scree plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n",
        "         pca.explained_variance_ratio_, 'bo-')\n",
        "plt.title('Scree Plot of Principal Components')\n",
        "plt.xlabel('Principal Components')\n",
        "plt.ylabel('Percentage of explained variances')\n",
        "plt.show()\n",
        "\n",
        "# Feature importance in components\n",
        "loadings = pd.DataFrame(\n",
        "    pca.components_.T,\n",
        "    columns=[f'PC{i+1}' for i in range(len(pca.components_))],\n",
        "    index=features.columns\n",
        ")\n",
        "\n",
        "# Top 5 features per component\n",
        "top_loadings = pd.DataFrame()\n",
        "for pc in loadings.columns:\n",
        "    top_5 = pd.DataFrame({\n",
        "        'feature': loadings.index,\n",
        "        'PC': pc,\n",
        "        'loading': loadings[pc]\n",
        "    })\n",
        "\n",
        "    top_5 = top_5.reindex(top_5['loading'].abs().sort_values(ascending=False).index)\n",
        "    top_5 = top_5.head(5)\n",
        "    top_loadings = pd.concat([top_loadings, top_5])\n",
        "\n",
        "\n",
        "\n",
        "print(top_loadings)\n",
        "\n",
        "\n",
        "# PC1 and 2\n",
        "pc12_loadings = top_loadings[top_loadings['PC'].isin(['PC1', 'PC2'])]\n",
        "pc12_loadings = pc12_loadings.sort_values(['PC', 'loading'],\n",
        "                                         ascending=[True, False])\n",
        "print(pc12_loadings)\n",
        "\n",
        "# PCA biplot\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.5)\n",
        "\n",
        "for i, feature in enumerate(features.columns):\n",
        "    plt.arrow(0, 0,\n",
        "              pca.components_[0, i]*max(abs(pca_result[:, 0])),\n",
        "              pca.components_[1, i]*max(abs(pca_result[:, 1])),\n",
        "              color='r', alpha=0.5)\n",
        "    plt.text(pca.components_[0, i]*max(abs(pca_result[:, 0]))*1.15,\n",
        "             pca.components_[1, i]*max(abs(pca_result[:, 1]))*1.15,\n",
        "             feature)\n",
        "\n",
        "plt.xlabel('First Principal Component')\n",
        "plt.ylabel('Second Principal Component')\n",
        "plt.title('PCA of Paper Concepts')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PF34txR9AW2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_clusters = 3\n",
        "pam = KMedoids(n_clusters=n_clusters,\n",
        "               random_state=42,\n",
        "               metric='euclidean',\n",
        "               method='alternate')\n",
        "cluster_labels = pam.fit_predict(pca_result[:, :5])\n",
        "\n",
        "# Get the medoid coordinates\n",
        "medoid_indices = pam.medoid_indices_\n",
        "medoid_coords = pca_result[medoid_indices, :5]\n",
        "\n",
        "# Function to get top papers for each centroid\n",
        "def get_top_papers(centroid_coord, pca_data, df, n_top=10):\n",
        "    distances = np.sum((pca_data - centroid_coord)**2, axis=1)\n",
        "    top_indices = np.argsort(distances)[:n_top]\n",
        "    return pd.DataFrame({\n",
        "        'cluster': [pam.medoid_indices_.tolist().index(medoid_indices[i]) + 1] * n_top,\n",
        "        'rank': range(1, n_top + 1),\n",
        "        'centroid_id': df['id'].iloc[top_indices].values,\n",
        "        'distance': distances[top_indices]\n",
        "    })\n",
        "\n",
        "# Get top papers for each centroid\n",
        "centroids_list = []\n",
        "for i, medoid_coord in enumerate(medoid_coords):\n",
        "    cluster_papers = get_top_papers(medoid_coord, pca_result[:, :5], df)\n",
        "    centroids_list.append(cluster_papers)\n",
        "\n",
        "centroids = pd.concat(centroids_list, ignore_index=True)\n",
        "\n",
        "centerpapers = pd.merge(centroids, metadata, left_on='centroid_id', right_on='id', how='left')"
      ],
      "metadata": {
        "id": "MCEQ282lix-6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "centerpapers.to_csv('center_papers.csv', index=False)"
      ],
      "metadata": {
        "id": "n8UdxBRXi0Bn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clustering with KMeans\n",
        "n_clusters = 3\n",
        "kmeans = KMeans(n_clusters=n_clusters,\n",
        "                random_state=42)\n",
        "cluster_labels = kmeans.fit_predict(pca_result[:, :5])\n",
        "\n",
        "# Get the centroids\n",
        "centroid_coords = kmeans.cluster_centers_\n",
        "\n",
        "# Function to get top papers for each centroid\n",
        "def get_top_papers(centroid_coord, pca_data, df, cluster_num, n_top=10):\n",
        "    distances = np.sum((pca_data - centroid_coord)**2, axis=1)\n",
        "    top_indices = np.argsort(distances)[:n_top]\n",
        "    return pd.DataFrame({\n",
        "        'cluster': [cluster_num + 1] * n_top,\n",
        "        'rank': range(1, n_top + 1),\n",
        "        'centroid_id': df['id'].iloc[top_indices].values,\n",
        "        'distance': distances[top_indices]\n",
        "    })\n",
        "\n",
        "# Get top papers for each centroid\n",
        "centroids_list = []\n",
        "for i in range(n_clusters):\n",
        "    cluster_papers = get_top_papers(centroid_coords[i], pca_result[:, :5], df, i)\n",
        "    centroids_list.append(cluster_papers)\n",
        "\n",
        "centroids = pd.concat(centroids_list, ignore_index=True)\n",
        "\n",
        "centerpapers = pd.merge(centroids, metadata, left_on='centroid_id', right_on='id', how='left')"
      ],
      "metadata": {
        "id": "XeB2uKAV5opy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Add visualization\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.scatter(pca_result[:, 0], pca_result[:, 2], c=cluster_labels, cmap='viridis', alpha=0.5)\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 2],\n",
        "           c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
        "plt.xlabel('First Principal Component')\n",
        "plt.ylabel('Second Principal Component')\n",
        "plt.title('PCA with K-means Clustering')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rn7L4Kov5qlN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}