{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stepanjaburek/quantum_social_science_lr/blob/main/stepan_colab_HCPC_13_1_25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA and Cluster Analysis"
      ],
      "metadata": {
        "id": "5lj-bQwjntRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "ugbp61azb7Il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn-extra\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from IPython.display import display\n",
        "\n",
        "data = pd.read_excel(\"/content/pdf_search_results(1).xlsx\")\n",
        "#data = pd.read_csv(\"/0_kw_analysis.csv\")\n",
        "metadata = pd.read_excel(\"/content/pdf_list(1).xlsx\")\n",
        "\n"
      ],
      "metadata": {
        "id": "_BTdRu2DyipG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data cleaning"
      ],
      "metadata": {
        "id": "0u69g6d6n1mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename \"Broekaert_2018\" in metadata\n",
        "metadata.loc[252, 'file_name'] = \"Broekaert_2018_The Tacit 'Quantum' of Meeting the Aesthetic Sign; Contextualize, Entangle,2.pdf\"\n",
        "data.loc[252, 'filename'] = \"Broekaert_2018_The Tacit 'Quantum' of Meeting the Aesthetic Sign; Contextualize, Entangle,2.pdf\"\n",
        "# Delete duplicit paper \"Yukalov et al. - 2018 - Information processing by networks of quantum deci.pdf\"\n",
        "data = data.drop(1170)\n",
        "# Delete paper \"Yilmaz - 2017 - Quantum cognition models of ethical decision-makin.pdf\" that is only present in data\n",
        "data = data.drop(1166)\n",
        "# Delete paper \"Park_2016_Decision-making &amp quantum mechanical models of cognitive processing.pdf\" that is only present in metadata\n",
        "metadata = metadata.drop(892)"
      ],
      "metadata": {
        "id": "LjkkYUF4i9DN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keywords"
      ],
      "metadata": {
        "id": "2l_j_UngjBe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exclude authors and create df\n",
        "df = data.drop(columns=[data.columns[i] for i in [1,2,4,16,30]])\n",
        "# Exclude social science fields\n",
        "df = df.drop(columns=df.columns[26:38])\n",
        "# Possibly also Exclude \"quantum\" and \"quantization\"\n",
        "df = df.drop(columns=df.columns[17:19])\n",
        "\n",
        "# keep only papers with some non-zero values in the final columns we want to keep\n",
        "df = df[df.iloc[:, 2:24].sum(axis=1) > 0]\n",
        "df = df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "jKsA4rIryes4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[(i, list(df.keys())[i]) for i in range(len(df.keys()))]"
      ],
      "metadata": {
        "id": "IjE0C6BJ-1ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "_oYwnmu94O5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Or keeep only the authors\n",
        "#df = data[data.columns[[1,2,4,16,30]]]"
      ],
      "metadata": {
        "id": "TLQa4ArZ4czP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Numbering the data for centroid extraction"
      ],
      "metadata": {
        "id": "5cGRGh4FjVGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metadata = metadata.reset_index()\n",
        "metadata = metadata.rename(columns={'index': 'id'})\n",
        "metadata['id'] = metadata['id'] + 1\n",
        "\n",
        "df = df.reset_index()\n",
        "df = df.rename(columns={'index': 'id'})\n",
        "df['id'] = df['id'] + 1"
      ],
      "metadata": {
        "id": "X5h0PIS6iknZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scaling in the rows"
      ],
      "metadata": {
        "id": "0C0uUN2ocb5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create features for the pca\n",
        "features = df.drop(['filename', 'id'], axis=1)\n",
        "feature_names = features.columns\n",
        "\n",
        "\n",
        "features = features.div(features.sum(axis=1), axis=0) # row normalization as Michael suggested, dividing the values by the sum of their row (paper)\n",
        "#features = StandardScaler().fit_transform(features.T).T # Z-score standardization on transposed data to work in rows\n",
        "\n",
        "features = pd.DataFrame(features, columns=feature_names)\n",
        "features"
      ],
      "metadata": {
        "id": "0WWcK4vtTneT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scaling in the columns"
      ],
      "metadata": {
        "id": "izhiH-2TUR22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create features for the pca\n",
        "features = df.drop(['filename', 'id'], axis=1)\n",
        "feature_names = features.columns\n",
        "\n",
        "# scale with either z-score standardization or MinMax normalization\n",
        "# features = StandardScaler().fit_transform(features) #Z-score\n",
        "# features = MinMaxScaler().fit_transform(features) # MinMax\n",
        "\n",
        "#features.iloc[:, 1:23] = (features.iloc[:, 1:23] > 0).astype(int) # binary >0\n",
        "#features.iloc[:, 1:23] = (features.iloc[:, 1:23] > 4).astype(int) # binary >4\n",
        "\n",
        "features = pd.DataFrame(features, columns=feature_names)\n",
        "features"
      ],
      "metadata": {
        "id": "2e2XrUxg5GV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data analysis"
      ],
      "metadata": {
        "id": "b8EjIT_YcyL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PCA"
      ],
      "metadata": {
        "id": "L9w7jKKnoH6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA()\n",
        "pca_result = pca.fit_transform(features)\n",
        "print(\"Explained variance ratio:\", pca.explained_variance_ratio_[:10])\n",
        "\n",
        "# Scree plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n",
        "         pca.explained_variance_ratio_, 'bo-')\n",
        "plt.title('Scree Plot of Principal Components')\n",
        "plt.xlabel('Principal Components')\n",
        "plt.ylabel('Percentage of explained variances')\n",
        "plt.show()\n",
        "\n",
        "# Feature importance in components\n",
        "loadings = pd.DataFrame(\n",
        "    pca.components_.T,\n",
        "    columns=[f'PC{i+1}' for i in range(len(pca.components_))],\n",
        "    index=features.columns\n",
        ")\n",
        "\n",
        "# Top 5 features per component\n",
        "top_loadings = pd.DataFrame()\n",
        "for pc in loadings.columns:\n",
        "    top_5 = pd.DataFrame({\n",
        "        'feature': loadings.index,\n",
        "        'PC': pc,\n",
        "        'loading': loadings[pc]\n",
        "    })\n",
        "\n",
        "    top_5 = top_5.reindex(top_5['loading'].abs().sort_values(ascending=False).index)\n",
        "    top_5 = top_5.head(5)\n",
        "    top_loadings = pd.concat([top_loadings, top_5])\n",
        "\n",
        "print(top_loadings)\n",
        "\n",
        "# PC1 and 2\n",
        "pc12_loadings = top_loadings[top_loadings['PC'].isin(['PC1', 'PC2', 'PC3', 'PC4'])]\n",
        "pc12_loadings = pc12_loadings.sort_values(['PC', 'loading'],\n",
        "                                         ascending=[True, False])\n",
        "print(pc12_loadings)\n",
        "\n",
        "# PCA biplot\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.5)\n",
        "\n",
        "for i, feature in enumerate(features.columns):\n",
        "    plt.arrow(0, 0,\n",
        "              pca.components_[0, i]*max(abs(pca_result[:, 0])),\n",
        "              pca.components_[1, i]*max(abs(pca_result[:, 1])),\n",
        "              color='r', alpha=0.5)\n",
        "    plt.text(pca.components_[0, i]*max(abs(pca_result[:, 0]))*1.15,\n",
        "             pca.components_[1, i]*max(abs(pca_result[:, 1]))*1.15,\n",
        "             feature)\n",
        "\n",
        "plt.xlabel('First Principal Component')\n",
        "plt.ylabel('Second Principal Component')\n",
        "plt.title('PCA of Paper Concepts')\n",
        "plt.show()\n",
        "display(pc12_loadings)"
      ],
      "metadata": {
        "id": "PF34txR9AW2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many PCs do we want? Literature says probably a sum between 80-90% of explained variance"
      ],
      "metadata": {
        "id": "zOzKjNdhMRSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
        "# Find number of components needed for 80% variance (we can change this)\n",
        "n_components_80 = np.argmax(cumulative_variance_ratio >= 0.8) + 1\n",
        "\n",
        "print(f\"Number of components needed to explain 80% of variance: {n_components_80}\")\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(cumulative_variance_ratio) + 1),\n",
        "         cumulative_variance_ratio,\n",
        "         'bo-')\n",
        "plt.axhline(y=0.8, color='r', linestyle='--', label='80% Threshold')\n",
        "plt.axvline(x=n_components_80, color='g', linestyle='--',\n",
        "            label=f'Components needed: {n_components_80}')\n",
        "plt.title('Cumulative Explained Variance Ratio')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nCumulative explained variance for first 10 components:\")\n",
        "for i in range(10):\n",
        "    print(f\"Components 1-{i+1}: {cumulative_variance_ratio[i]:.3f}\")"
      ],
      "metadata": {
        "id": "7dFNNsTQFyRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hierarchical Clustering on Principal Components (HCPC)"
      ],
      "metadata": {
        "id": "aYBuz43aIZQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
        "\n",
        "# 1. Define how many PCs to use\n",
        "n_components = 7 # I think we should use between 7 and 9 based on the PCA\n",
        "selected_components = pca_result[:, :n_components]\n",
        "\n",
        "# 2. Hierarchical clustering uisng Ward\n",
        "linked = linkage(selected_components, method='ward') # the methods can be different. Ward method is the one we use in Ipsos and the typical choice in the literature\n",
        "\n",
        "# Plot dendrogram\n",
        "plt.figure(figsize=(12, 8))\n",
        "dendrogram(linked, truncate_mode='lastp', p=12)\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Distance')\n",
        "plt.show()\n",
        "\n",
        "# 3. Get initial clusters from hierarchical clustering # optional step though, we dont really need them\n",
        "n_clusters = 4\n",
        "hc_labels = fcluster(linked, n_clusters, criterion='maxclust')\n",
        "\n",
        "# 4. K-means to finalize things\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "kmeans.fit(selected_components)\n",
        "final_labels = kmeans.labels_\n",
        "\n",
        "\n",
        "# Visualize final clusters\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1],\n",
        "                     c=final_labels, cmap='viridis')\n",
        "centroids = kmeans.cluster_centers_\n",
        "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
        "           c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
        "plt.colorbar(scatter)\n",
        "plt.title('HCPC Final Clusters')\n",
        "plt.xlabel('First Principal Component')\n",
        "plt.ylabel('Second Principal Component')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EMLnGW5fIvE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Medoids as an alternative ending"
      ],
      "metadata": {
        "id": "zQfUjvfQbmn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmedoids = KMedoids(n_clusters=n_clusters,\n",
        "                    random_state=42,\n",
        "                    metric='euclidean')\n",
        "kmedoids.fit(selected_components)\n",
        "final_labels = kmedoids.labels_\n",
        "\n",
        "# Visualize final clusters\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1],\n",
        "                     c=final_labels, cmap='viridis')\n",
        "\n",
        "# Get medoid locations in PCA space\n",
        "medoid_indices = kmedoids.medoid_indices_\n",
        "medoid_locations = pca_result[medoid_indices]\n",
        "\n",
        "# Plot medoids\n",
        "plt.scatter(medoid_locations[:, 0], medoid_locations[:, 1],\n",
        "           c='red', marker='x', s=200, linewidths=3, label='Medoids')\n",
        "\n",
        "plt.colorbar(scatter)\n",
        "plt.title('HCPC Final Clusters (K-medoids)')\n",
        "plt.xlabel('First Principal Component')\n",
        "plt.ylabel('Second Principal Component')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ybGoRg4lbAhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get central papers from the HCPC"
      ],
      "metadata": {
        "id": "BRzBioJdaGj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "centroids_list = []\n",
        "for i in range(n_clusters):\n",
        "    distances = np.sum((selected_components - kmeans.cluster_centers_[i])**2, axis=1)\n",
        "    top_indices = np.argsort(distances)[:10]  # specify N of papers\n",
        "\n",
        "    cluster_papers = pd.DataFrame({\n",
        "        'cluster': [i + 1] * 10,\n",
        "        'rank': range(1, 11),\n",
        "        'centroid_id': df['id'].iloc[top_indices].values,\n",
        "        'distance': distances[top_indices]\n",
        "    })\n",
        "    centroids_list.append(cluster_papers)\n",
        "\n",
        "centroids = pd.concat(centroids_list, ignore_index=True)\n",
        "centerpapers = pd.merge(centroids, metadata, left_on='centroid_id', right_on='id', how='left')\n",
        "centerpapers.to_csv('means_center_papers_hcpc.csv', index=False)"
      ],
      "metadata": {
        "id": "kNrUtow2KsGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "centroids_list = []\n",
        "for i in range(n_clusters):\n",
        "    medoid_point = selected_components[kmedoids.medoid_indices_[i]]\n",
        "    distances = np.sum((selected_components - medoid_point)**2, axis=1)\n",
        "    top_indices = np.argsort(distances)[:10]  # specify N of papers\n",
        "\n",
        "    cluster_papers = pd.DataFrame({\n",
        "        'cluster': [i + 1] * 10,\n",
        "        'rank': range(1, 11),\n",
        "        'centroid_id': df['id'].iloc[top_indices].values,\n",
        "        'distance': distances[top_indices]\n",
        "    })\n",
        "    centroids_list.append(cluster_papers)\n",
        "\n",
        "centroids = pd.concat(centroids_list, ignore_index=True)\n",
        "centerpapers = pd.merge(centroids, metadata, left_on='centroid_id', right_on='id', how='left')\n",
        "centerpapers.to_csv('medoids_center_papers_hcpc.csv', index=False)"
      ],
      "metadata": {
        "id": "waIlMfyCbXOt"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}